# LSTM

## 反向求导

![](../../../.gitbook/assets/image%20%2810%29.png)

假设在求一个语言模型，那么总的损失是：

$$
E=\frac{1}{2} \sum_{t=1}^T E_t
$$

在t时刻的梯度为：

![](../../../.gitbook/assets/image%20%289%29.png)

> 备注: 有的文章可能提到在求某个时刻的梯度时，即要求来自下一个时刻的梯度，有要求当前时刻输出的梯度
>
> 这里这么理解应该是靠谱的，为方便描述，将上边图片的方式记录为method1， 将上边提到的求两个源的梯度的方法记录为method2。
>
> 在method1 中，我们可以看到， 它其实是将所有时刻的梯度累加起来，例如有1，2，3个时刻，假设di,j 表示第i个时刻损失对第j个时刻的参数求导，在第3个时刻的梯度就是：d33+d32+d31; 在第2个时刻的梯度是： d22+d21; 在第1个时刻的梯度是：d11， 最终累加所有时刻：d33+d32+d31+d22+d21+d11表示全部序列产生的梯度。 
>
> 这里可能会有疑问，为什么在计算每个时刻的时候，不去管前边时刻的输出损失？例如在求d21的时候，对第一个时刻来说，只关注到来自第2个时刻的损失，并没有关注到来自第1个时刻自身输出的损失。原因是 当前计算的是第2个时刻的损失并计算梯度，这个时刻的损失和前边第1个时刻的损失没有什么关系，当然也就不需要加上第1个时刻的损失缠身的梯度了。 第一个时刻自身输出产生的梯度会使用第一个时刻的损失继续求梯度，即上边提到的d11。
>
> 在 method2中，它在计算第3个时刻对第2个时刻参数梯度的时候，即d32，即考虑到了来自第3个时刻的梯度，同时又考虑到第2个时刻自身产生的梯度，即d\_out2/d\_h2,  然后将两者相加作为对当前时刻的梯度。这样一次反向传播就相当于计算完成了整个序列反向传播，计算效率会更高。
>
> 因此，个人认为这是实现rnn反向传播的不同描述，本质上是一致的，只不过前者更偏理论一点，后者更偏实践一点。

