# sigmoid

### sigmoid函数长个什么样

下边这张图就是sigmoid函数图像，是一条类"S"型曲线，值域为\(0, 1\)。当输入值偏大的时候，函数取值比较容易接近1；当输入值偏小的时候，函数取值比较容易接近0。

这说明sigmoid函数倾向将输入数值"挤压"到1或者0。

![](../../.gitbook/assets/image%20%285%29.png)

sigmoid函数数学表达式是这样的：

$$
f(x) = \frac{1}{1+e^{-x}}
$$

### sigmoid函数有哪些优势

#### 1. sigmoid函数的输出可以看作概率

从函数图像容易看出，sigmoid函数将输入数值映射到0和1之间，相当于是对数值进行了归一化。因此这些数值可以被看作是概率。

**举个例子**，我们在做一个二分类任务：识别一张图片是猫是狗。不妨假设猫对应标签0，狗对应标签1。

我们在将这张图片传入sigmoid函数之后就会得到一个0和1之间的概率，如果这个概率大于0.5，那这张图片是狗的可能性更大一点，否则，这张图片就应该是猫。

> **备注**：机器学习的经典模型**Logitstic回归**就是按照这个逻辑设计的，有兴趣的同学去看看。

#### 2. sigmoid函数的输出可以被当做"门"，控制信息的传递

sigmoid函数将输入数值映射到0和1之间，因此可以控制信息的传递。0与一个数字相乘为0，代表该数字不能被通过；1与一个数字相乘为该数字本身，代表该数字可以无损通过。

举个例子，假设我们如下两个矩阵，一个是信息矩阵，一个是门矩阵。其中这个门矩阵是经过sigmoid处理后的矩阵\(将原始矩阵中的每个数字传入sigmoid，将原始数值映射为0和1之间\)。

我们可以看到原始的信息矩阵和门矩阵相乘之后，各个位置有着不同的效果。有的数值能够无损通过\(绿色部分\)，有的数值则完全不能通过\(红色部分\)，有的则只能通过一部分\(黄色部分\)。

因此门矩阵能够起到信息筛选或过滤的作用。

> **备注**：有着NLP背景的同学应该对此比较熟悉，长短时记忆网络LSTM中的**门单元**用的就是这个原理。

![](../../.gitbook/assets/image%20%287%29.png)

#### 3. 整条曲线比较平滑，sigmoid函数是全线条可微的。

### sigmoid函数有哪些缺点

#### 1. 容易导致梯度消失

上边我们提到，sigmoid函数比较容易将输入数值"挤压"到0或者1。从图像上来看，就是曲线随着输入数值的增大或者减小比较容易达到平稳，这不利于梯度的反向传播。

观察图像上曲线的平稳部分，这部分的曲线斜率接近于0，因此反向传播时的梯度接近于0，这将不利于参数的更新，模型的学习。

#### 2. sigmoid函数执行指数运算，计算成本高

#### 3. sigmoid函数不是以0为中心，这会降低权重更新的效率

知道很多同学对这一点会一脸懵逼，请参考这位同学写的，非常清楚。

{% embed url="https://liam.page/2018/04/17/zero-centered-active-function/" %}



