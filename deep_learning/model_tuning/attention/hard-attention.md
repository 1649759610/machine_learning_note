# 经典注意力机制

## 1. 用机器翻译任务带你看Attention机制的计算

单独地去讲**Attention机制**会有些抽象，也有些枯燥，所以我们不妨以**机器翻译**任务为例，通过讲解**Attention机制**在机器翻译任务中的应用方式，来了解**Attention机制**的使用。

如果你对机器翻译任务还不是很熟悉，那也没关系，因为现在我会告诉你机器翻译是什么样的任务。以**中译英**为例，机器翻译是将一串中文语句翻译为对应的英文语句，如**图1**所示。

**图1**展示了一种经典的机器翻译结构**Seq-to-Seq**，并且向其中添加了**Attention计算**。**Seq-to-Seq**结构包含两个部分：Encoder和Decoder。其中Encoder用于将中文语句进行编码，这些编码后续将提供给Decoder进行使用；Decoder将根据Decoder的数据进行解码。我们还是以图1为例详细解释一下Decoder的解码过程。

更明确的讲，**图1**展示的是生成单词"machine"时的计算方式。首先将前一个时刻的输出状态 $$q_2$$ 和Encoder的输出 $$h=[h_1,h_2,h_3,h_4]$$ 进行Attention计算，得到一个当前时刻的 $$context$$ ，用公式可以这样组织：

$$
[a_1,a_2,a_3,a_4]= softmax([s(q_2, h_1), s(q_2,h_2),s(q_2, h_3),s(q_2, h_4)]) \\context=\sum_{i=1}^4 a_i \cdot h_i
$$

我们来解释一下，这里的 $$s(q_i,h_j)$$ 表示注意力打分函数，它是个标量，其大小描述了当前时刻在这些Encoder的结果上的关注程度，这个函数在后边会展开讨论。然后用softmax对这个结果进行归一化，最后使用加权评价获得当前时刻的上下文向量 $$context$$。这个context可以解释为：截止到当前已经有了"I love"，在此基础上下一个时刻应该更加关注**源中文语句**的那些内容。这就是关于**Attention机制**的一个完整计算。

最后，将这个$$context$$和上个时刻的输出"love"进行融合作为当前时刻RNN单元的输入。

{% hint style="info" %}
**图1**中采用了继续融合上一步的输出结果，例如上述描述中融合了"love"，在有些实现中，并没有融入这个上一步的输出，默认 $$q_2$$ 中已经携带了"love"的信息，这也是合理的。

个人认为，显示地融合上一步的输出，能够进一步加深网络对当前已有知识的印象，帮助网络进一步明确此刻的输出结果。
{% endhint %}

![&#x56FE;1 &#x673A;&#x5668;&#x7FFB;&#x8BD1;&#x793A;&#x4F8B;&#x56FE;](../../../.gitbook/assets/image%20%2815%29.png)

## 2. 注意力机制的正式表述

前边我们通过机器翻译任务介绍了**Attention机制**的整体计算。但是还有点**小尾巴**没有展开，就是那个**注意力打分函数**的计算，现在我们将来讨论这个事情。但在讲这个函数之前，我们先来对上边的**Attention机制**的计算做个总结。

假设现在我们要对一组输入 $$H=[h_1,h_2,h_3,...,h_n]$$ 使用**Attention机制**计算重要的内容，那我们要有一个**查询向量** $$q$$\(这个向量往往和你做的任务有关，比如机器翻译中用到的那个 $$q_2$$ \) 



