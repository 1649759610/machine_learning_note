# 学习率

## 1. 学习率对网络的影响

在模型训练过程中，我们通常设置的求解目标是使损失函数 $$J(\theta)$$ 最小化。为了达到这个目标，一般会采用向负梯度方向更新参数，对应参数更新公式可写为：

$$
\theta = \theta - \eta \frac{\partial J(\theta)}{\partial \theta}
$$

其中学习率用 $$\eta$$ 表示，可以看到学习率控制网络中参数更新的快慢。

如果学习率$$\eta$$较大，那么参数的更新速度就会很快，可以加快网络的收敛速度，但如果学习率过大，可能会导致参数在最优解附近震荡，损失函数难以收敛，甚至可能会错过最优解，导致参数向错误的方向更新，损失函数不仅不收敛反而可能爆炸（如**图1a**所示）。

如果学习率$$\eta$$较小，网络可能不会错过最优点，但是网络学习速度会变慢。同时，如果学习率过小，则很可能会陷入局部最优点（如**图1b**所示）。

因此，只有找到合适的学习率，才能使得网络较快的速度收敛到较好的解。

![&#x56FE;1 &#x5B66;&#x4E60;&#x7387;&#x66F4;&#x65B0;&#x7684;&#x4E24;&#x79CD;&#x60C5;&#x51B5;](../../.gitbook/assets/image%20%2824%29.png)

## 2. Warmup预热学习率

模型刚开始训练时，模型中参数都是随机初始化的，此时若选择一个较大的学习率,可能带来模型的**不稳定\(振荡\)**，选择Warmup预热学习率的方式，可以使得开始训练的几个epoches或者一些steps内学习率较小，在预热的小学习率下，模型可以慢慢趋于稳定,等模型相对稳定后再选择预先设置的学习率进行训练，使得**模型收敛速度变得更快，模型效果更佳**。

一般Warmup按照如下的方式进行更新学习率：

$$
learning\_rate = init\_learning\_rate * (\frac{global\_step}{warmup\_steps})
$$

模型开始训练时，学习率在warmup\_steps内逐步上升_，_warmup\_steps结束后，学习率将恢复初始设定的学习率。

## 3. 衰减方式

### 3.1 分段常数衰减

​分段常数衰减需要事先定义好的训练次数区间，在对应区间置不同的学习率的常数值，一般情况刚开始的学习率要大一些，之后要越来越小，要根据样本量的大小设置区间的间隔大小，样本量越大，区间间隔要小一点。下图即为分段常数衰减的学习率变化图，横坐标代表训练次数，纵坐标代表学习率。

![&#x56FE;2 &#x5206;&#x6BB5;&#x5E38;&#x6570;&#x8870;&#x51CF;](../../.gitbook/assets/image%20%2823%29.png)

### 3.2 指数衰减

以指数衰减方式进行学习率的更新，学习率的大小和训练次数指数相关，其更新规则为：

$$
decayed\_learning\_rate = learning\_rate * {decay\_rate}^{\frac {global\_step}{decay\_steps}}
$$

这种衰减方式简单直接，收敛速度快，是最常用的学习率衰减方式，如**图3**所示，绿色的为学习率随训练次数的指数衰减方式，红色的即为分段常数衰减，它在一定的训练区间内保持学习率不变。

![&#x56FE;3 &#x6307;&#x6570;&#x8870;&#x51CF;](../../.gitbook/assets/image%20%2821%29.png)

### 3.3 自然指数衰减

与指数衰减方式相似，不同的在于它的衰减底数是 $$e$$ ，故而其收敛的速度更快，一般用于相对比较容易训练的网络，便于较快的收敛，其更新规则如下 ：

$$
decayed\_learning\_rate =learning\_rate*e^{{-decay\_rate}*{global\_step}}
$$

图4为分段常数衰减、指数衰减、自然指数衰减三种方式的对比图，红色的即为分段常数衰减图，阶梯型曲线。蓝色线为指数衰减图，绿色即为自然指数衰减图，可以看到自然指数衰减方式下的学习率衰减程度要大于一般指数衰减方式，有助于更快的收敛。

![&#x56FE;4 &#x81EA;&#x7136;&#x6307;&#x6570;&#x8870;&#x51CF;](../../.gitbook/assets/image%20%2820%29.png)

### 3.4 多项式衰减

​ 应用多项式衰减的方式进行更新学习率，这里会给定初始学习率和最低学习率取值，然后将会按照给定的衰减方式将学习率从初始值衰减到最低值,其更新规则如下式所示：

$$
global\_step=min(global\_step, decay\_steps)
$$

$$
decayed\_learning\_rate = (learning\_rate - end\_learning\_rate)  \\
* (1-\frac{global\_step}{decay\_steps})^{power} + end\_learning\_rate
$$

​ 需要注意的是，这里有两个机制，降到最低学习率后，到训练结束可以一直使用最低学习率进行更新，另一个是再次将学习率调高，使用 decay\_steps 的倍数，取第一个大于 global\_steps 的结果，如下式所示:

$$
decayed\_learning\_rate =decay\_steps*ceil(\frac{global\_step}{decay\_steps})
$$

它是用来防止神经网络在训练的后期由于学习率过小而导致的网络一直在某个局部最小值附近震荡，这样可以通过在后期增大学习率跳出局部极小值。如**图5**所示，红色线代表学习率降低至最低后，一直保持学习率不变进行更新，绿色线代表学习率衰减到最低后，又会再次循环往复的升高降低。

![](../../.gitbook/assets/image%20%2822%29.png)

## 4. 文献参考

1. [warmup预热学习率](https://www.cnblogs.com/shona/p/12252940.html)
2. [深度学习500问](https://book.douban.com/subject/35307932/)



